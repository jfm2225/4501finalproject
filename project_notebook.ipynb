{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32f8ca24",
   "metadata": {},
   "source": [
    "# Understanding Hired Rides in NYC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25627e8d",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "_A checklist of requirements to keep you on track. Remove this whole cell before submitting the project._\n",
    "\n",
    "* Code clarity: make sure the code conforms to:\n",
    "    * [ ] [PEP 8](https://peps.python.org/pep-0008/) - You might find [this resource](https://realpython.com/python-pep8/) helpful as well as [this](https://github.com/dnanhkhoa/nb_black) or [this](https://jupyterlab-code-formatter.readthedocs.io/en/latest/) tool\n",
    "    * [ ] [PEP 257](https://peps.python.org/pep-0257/)\n",
    "    * [ ] Break each task down into logical functions\n",
    "* The following files are submitted for the project (see the project's GDoc for more details):\n",
    "    * [ ] `README.md`\n",
    "    * [ ] `requirements.txt`\n",
    "    * [ ] `.gitignore`\n",
    "    * [ ] `schema.sql`\n",
    "    * [ ] 6 query files (using the `.sql` extension), appropriately named for the purpose of the query\n",
    "    * [x] Jupyter Notebook containing the project (this file!)\n",
    "* [x] You can edit this cell and add a `x` inside the `[ ]` like this task to denote a completed task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f75fd94",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6fb3827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: geopandas in c:\\users\\johnf\\anaconda3\\lib\\site-packages (0.12.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\johnf\\anaconda3\\lib\\site-packages (from geopandas) (21.3)\n",
      "Requirement already satisfied: fiona>=1.8 in c:\\users\\johnf\\anaconda3\\lib\\site-packages (from geopandas) (1.8.22)\n",
      "Requirement already satisfied: shapely>=1.7 in c:\\users\\johnf\\anaconda3\\lib\\site-packages (from geopandas) (1.8.5.post1)\n",
      "Requirement already satisfied: pandas>=1.0.0 in c:\\users\\johnf\\anaconda3\\lib\\site-packages (from geopandas) (1.5.2)\n",
      "Requirement already satisfied: pyproj>=2.6.1.post1 in c:\\users\\johnf\\anaconda3\\lib\\site-packages (from geopandas) (3.4.0)\n",
      "Requirement already satisfied: six>=1.7 in c:\\users\\johnf\\anaconda3\\lib\\site-packages (from fiona>=1.8->geopandas) (1.16.0)\n",
      "Requirement already satisfied: click-plugins>=1.0 in c:\\users\\johnf\\anaconda3\\lib\\site-packages (from fiona>=1.8->geopandas) (1.1.1)\n",
      "Requirement already satisfied: munch in c:\\users\\johnf\\anaconda3\\lib\\site-packages (from fiona>=1.8->geopandas) (2.5.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\johnf\\anaconda3\\lib\\site-packages (from fiona>=1.8->geopandas) (63.4.1)\n",
      "Requirement already satisfied: click>=4.0 in c:\\users\\johnf\\anaconda3\\lib\\site-packages (from fiona>=1.8->geopandas) (8.0.4)\n",
      "Requirement already satisfied: attrs>=17 in c:\\users\\johnf\\anaconda3\\lib\\site-packages (from fiona>=1.8->geopandas) (21.4.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\johnf\\anaconda3\\lib\\site-packages (from fiona>=1.8->geopandas) (2022.9.14)\n",
      "Requirement already satisfied: cligj>=0.5 in c:\\users\\johnf\\anaconda3\\lib\\site-packages (from fiona>=1.8->geopandas) (0.7.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\johnf\\anaconda3\\lib\\site-packages (from pandas>=1.0.0->geopandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\users\\johnf\\anaconda3\\lib\\site-packages (from pandas>=1.0.0->geopandas) (1.21.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\johnf\\anaconda3\\lib\\site-packages (from pandas>=1.0.0->geopandas) (2022.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\johnf\\anaconda3\\lib\\site-packages (from packaging->geopandas) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\johnf\\anaconda3\\lib\\site-packages (from click>=4.0->fiona>=1.8->geopandas) (0.4.5)\n",
      "Requirement already satisfied: fastparquet in c:\\users\\johnf\\anaconda3\\lib\\site-packages (2022.11.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\johnf\\anaconda3\\lib\\site-packages (from fastparquet) (21.3)\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\users\\johnf\\anaconda3\\lib\\site-packages (from fastparquet) (1.21.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\johnf\\anaconda3\\lib\\site-packages (from fastparquet) (2022.7.1)\n",
      "Requirement already satisfied: cramjam>=2.3 in c:\\users\\johnf\\anaconda3\\lib\\site-packages (from fastparquet) (2.6.2)\n",
      "Requirement already satisfied: pandas>=1.5.0 in c:\\users\\johnf\\anaconda3\\lib\\site-packages (from fastparquet) (1.5.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\johnf\\anaconda3\\lib\\site-packages (from pandas>=1.5.0->fastparquet) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\johnf\\anaconda3\\lib\\site-packages (from pandas>=1.5.0->fastparquet) (2.8.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\johnf\\anaconda3\\lib\\site-packages (from packaging->fastparquet) (3.0.9)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\johnf\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas>=1.5.0->fastparquet) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install geopandas\n",
    "!pip install fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66dcde05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from math import sin, cos, sqrt, atan2, radians\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sqlalchemy as db\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import re\n",
    "import fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f1242c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAXI_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "# add other constants to refer to any local data, e.g. uber & weather\n",
    "UBER_CSV = \"uber_rides_sample.csv\"\n",
    "\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\"\n",
    "TAXI_SHPFILE = \"taxi_zones.shp\"\n",
    "WEATHER_DATA_FILES = [\"2009_weather.csv\", \"2010_weather.csv\",\"2011_weather.csv\",\n",
    "                      \"2012_weather.csv\",\"2013_weather.csv\",\"2014_weather.csv\",\"2015_weather.csv\" ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad10ea",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf38168",
   "metadata": {},
   "source": [
    "_A checklist of requirements to keep you on track. Remove this whole cell before submitting the project. The order of these tasks aren't necessarily the order in which they need to be done. It's okay to do them in an order that makes sense to you._\n",
    "\n",
    "\n",
    "* Remove invalid data points (take a moment to consider what's invalid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32074561",
   "metadata": {},
   "source": [
    "### Calculating distance\n",
    "The below function calculates the distance between two coordinates from degrees to kilometers that utilizes the math module. It converts pickup and dropoff latitude and longitude coordinates from degrees to radians because all trig functions in the math module use radianns, calculates distance between two coordinates and uses a formula to convert distance in radians to distance in kilometers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4cbbe6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance(df):\n",
    "    \"\"\"Converts geographical coordinates from a dataframe into a list of distances in kilometers.\"\"\"\n",
    "    \n",
    "    # Convert the coordinates into from degrees for use of trig functions.\n",
    "    lat1 = []\n",
    "    for coord in df[\"pickup_longitude\"]: \n",
    "        lat1.append(radians(coord))\n",
    "    \n",
    "    lon1 = []\n",
    "    for coord in df[\"pickup_latitude\"]: \n",
    "        lon1.append(radians(coord))\n",
    "\n",
    "    lat2 = []\n",
    "    for coord in df[\"dropoff_longitude\"]: \n",
    "        lat2.append(radians(coord))\n",
    "    \n",
    "    lon2 = []\n",
    "    for coord in df[\"dropoff_latitude\"]: \n",
    "        lon2.append(radians(coord))\n",
    "        \n",
    "    \n",
    "    # Calculate difference in between coordinates. \n",
    "    diflon = list()\n",
    "    for item1, item2 in zip(lon1, lon2): \n",
    "        diflon.append(item2 - item1)\n",
    "\n",
    "    diflat = list()\n",
    "    for item1, item2 in zip(lat1, lat2): \n",
    "        diflat.append(item2 - item1)  \n",
    "        \n",
    "        \n",
    "    # Calculating distance using the quadratic formula.\n",
    "    R = 6373.0\n",
    "    dist = list()\n",
    "    for rad1, rad2, item1, item2 in zip(diflon, diflat, lat1, lat2):\n",
    "        a = sin(rad2 / 2)**2 + cos(item1) * cos(item2) * sin(rad1 / 2)**2\n",
    "        c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "        distance = R * c \n",
    "        dist.append(distance)\n",
    "        \n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d6abf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_distance_column(df):\n",
    "    \"\"\" Assigns distance values to its own column in a dataframe \"\"\"\n",
    "    df[\"distance\"] = calculate_distance(df)\n",
    "    return df "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094b4d6d",
   "metadata": {},
   "source": [
    "### Processing Uber Data\n",
    "\n",
    "Processing data before analyzing it is necessary to create a more efficient analysis. This function removes unnecessary columns, removes invalid data points, normalizes column names so that analyses later on can be generalized to a common name, and filters data to only show trips in a certain coordinate limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dac2f488",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_uber_data(csv_file):\n",
    "    \"\"\"Imports and reads the csv file of Uber data.\"\"\"\n",
    "    UBER_DATA = pd.read_csv(csv_file) \n",
    "    \n",
    "    # Drop unnecessary columns from the dataset.\n",
    "    UBER_DATA = UBER_DATA[[\"pickup_datetime\", \"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\"]]\n",
    "    \n",
    "    # Drop rows from the dataset with invalid coordinate values.\n",
    "    UBER_DATA = UBER_DATA[~(UBER_DATA[['pickup_longitude','pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']] == 0).any(axis=1)] \n",
    "    \n",
    "    # Define valid bounds for NYC coordinates.\n",
    "    westlimit = -74.242330\n",
    "    eastlimit = -73.717047\n",
    "\n",
    "    southlimit = 40.560445\n",
    "    northlimit = 40.908524\n",
    "    \n",
    "    # Fetch the coordinate values from the dataset.\n",
    "    pickup_longitude = UBER_DATA['pickup_longitude'].values\n",
    "    pickup_latitude = UBER_DATA['pickup_latitude'].values\n",
    "    \n",
    "    # Create filters for data within longitude limits.\n",
    "    pickupwest = pickup_longitude >= westlimit\n",
    "    pickupeast = pickup_longitude <= eastlimit \n",
    "    pickuplon = pickupwest * pickupeast \n",
    "    \n",
    "    # Create filters for data within latitude limits.\n",
    "    pickupnorth = pickup_latitude <= northlimit \n",
    "    pickupsouth = pickup_latitude >= southlimit\n",
    "    pickuplat = pickupnorth * pickupsouth \n",
    "    \n",
    "    # Create the final pickup filter.\n",
    "    pickupfilter = pickuplon * pickuplat\n",
    "        \n",
    "\n",
    "    # Filter out dropoff data.\n",
    "    dropoff_longitude = UBER_DATA['dropoff_longitude'].values\n",
    "    dropoff_latitude = UBER_DATA['dropoff_latitude'].values\n",
    "    \n",
    "    # Create filters for data within longitude limits.\n",
    "    dropoffwest = dropoff_longitude >= westlimit\n",
    "    dropoffeast = dropoff_longitude <= eastlimit \n",
    "    dropofflon = dropoffwest * dropoffeast \n",
    "    \n",
    "    # Create filters for data within latitude limits.\n",
    "    dropoffnorth = dropoff_latitude <= northlimit \n",
    "    dropoffsouth = dropoff_latitude >= southlimit\n",
    "    dropofflat = dropoffnorth * dropoffsouth \n",
    "    \n",
    "    # Create final dropoff filter.\n",
    "    dropofffilter = dropofflon * dropofflat\n",
    "    \n",
    "    # Create final boundary filter.\n",
    "    finalfilter = pickupfilter * dropofffilter\n",
    "    \n",
    "    # Create the final filtered dataframe.\n",
    "    UBER_DATA = UBER_DATA[finalfilter]\n",
    "    \n",
    "    # Normalize column datatypes.\n",
    "    UBER_DATA[\"pickup_datetime\"]=pd.to_datetime(UBER_DATA[\"pickup_datetime\"])\n",
    "\n",
    "    return UBER_DATA\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f836f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uber_data():\n",
    "    \"\"\" Loads uber dataframe and uses add_distance function to calculate distance \"\"\"\n",
    "    uber_dataframe = load_and_clean_uber_data(UBER_CSV)\n",
    "    add_distance_column(uber_dataframe)\n",
    "    return uber_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93daa717",
   "metadata": {},
   "source": [
    "### Processing Taxi Data\n",
    "\n",
    "Processing data before analyzing it is necessary to create a more efficient analysis. The first function [find_taxi_csv_urls()] scrapes the provided taxi database url to collect the links for all other taxi data files. It uses regex to match a format of the desired files and append the links to a list. It then selects the link from the requested time period and puts the links in chronological order. The functions named get_and_clean_month_taxi_data() remove unnecessary columns, removes invalid data points, normalizes column names so that analyses later on can be generalized to a common name, filter data to only show trips in a certain coordinate limit, and randomly sample from the set so that the size of the final taxi dataset is comparable to that of the uber dataset. The format of the taxi dataframes were not normalized therefore, three different functions were created in order to accomodate the different formats. The last function [get_and_clean_taxi_data()] merges all of the dataframes into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cbd0d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_taxi_csv_urls():\n",
    "    \"\"\"Fetches the download URLs from the NYC data source site.\"\"\"\n",
    "    response = requests.get(TAXI_URL)\n",
    "    response_page = BeautifulSoup(response.content, \"lxml\")\n",
    "    elements = response_page.find_all('a', href=True)\n",
    "\n",
    "    list1 = []\n",
    "    for ele in elements: \n",
    "        list1.append(ele['href'])\n",
    "        \n",
    "    # Use regex to math to desired format of the datafiles.\n",
    "    links = []\n",
    "    for i in range(455): \n",
    "        string = str(list1[i])\n",
    "        pattern = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_20\\d\\d-\\d\\d.parquet\"\n",
    "        if re.search(pattern, string) is not None: \n",
    "            links.append(string)\n",
    "            \n",
    "    # Get only the links from the requested time period.\n",
    "    links = links[80:]\n",
    "    \n",
    "    # Put links in chronological order.\n",
    "    year15 = [] \n",
    "    months = ['-01','-02','-03','-04','-05','-06']# Fetch data only through June of 2015.\n",
    "    year14 = []\n",
    "    year13 = []\n",
    "    year12 = []\n",
    "    year11 = []\n",
    "    year10 = []\n",
    "    year09 = []\n",
    "    \n",
    "    for link in links:\n",
    "        if '2009' in link:\n",
    "            year09.append(link)\n",
    "        elif '2010' in link:\n",
    "            year10.append(link)\n",
    "        elif '2011' in link:\n",
    "            year11.append(link)\n",
    "        elif '2012' in link:\n",
    "            year12.append(link)\n",
    "        elif '2012' in link:\n",
    "            year12.append(link)\n",
    "        elif '2013' in link:\n",
    "            year13.append(link)\n",
    "        elif '2014' in link:\n",
    "            year14.append(link)\n",
    "        elif '2015' in link:\n",
    "            for month in months:\n",
    "                if month in link:\n",
    "                    year15.append(link)\n",
    "    \n",
    "    final_links = year09 + year10 + year11 + year12 + year13 + year14 + year15\n",
    "   \n",
    "    return final_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "92a25487",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_taxi_data(links):\n",
    "    \"\"\"Download the parquet data files using the links.\"\"\"\n",
    "    for link in links:\n",
    "        filename = link[48:]\n",
    "        response = requests.get(link)\n",
    "        open(filename,'wb').write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "403b422a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_month_taxi_data1(url):\n",
    "    \"\"\"Filter and clean the taxi data for analysis purposes.\"\"\"\n",
    "    \n",
    "    # Use regex to math to desired format of the datafiles.\n",
    "    pattern1 = \"yellow_tripdata_20\\d\\d-\\d\\d.parquet\"\n",
    "    if re.search(pattern1, url) is not None: \n",
    "        filename = re.search(pattern1, url).group()\n",
    "    \n",
    "    dataset = pd.read_parquet(filename, engine='fastparquet')\n",
    "    \n",
    "    dataset = dataset[[\"Trip_Pickup_DateTime\", \"Start_Lon\", \"Start_Lat\", \"End_Lon\", \"End_Lat\"]]\n",
    "    dataset = dataset.set_axis([\"pickup_datetime\", \"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\"], axis=1, inplace=False)\n",
    "    df1 = dataset\n",
    "    \n",
    "    # Filter out rows with invalid coordinates.\n",
    "    dataset = dataset[~(dataset[['pickup_longitude','pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']] == 0).any(axis=1)] \n",
    "    \n",
    "    # Define longitude and latitude bounds. \n",
    "    westlimit = -74.242330\n",
    "    eastlimit = -73.717047\n",
    "  \n",
    "    southlimit = 40.560445\n",
    "    northlimit = 40.908524\n",
    "    \n",
    "    # Filter out pickup data. \n",
    "    tpickup_longitude = dataset['pickup_longitude'].values\n",
    "    tpickup_latitude = dataset['pickup_latitude'].values\n",
    "    \n",
    "    # Create filters for data within longitude limits.\n",
    "    tpickupwest = tpickup_longitude >= westlimit\n",
    "    tpickupeast = tpickup_longitude <= eastlimit \n",
    "    tpickuplon = tpickupwest * tpickupeast \n",
    "    \n",
    "    # Create filters for data within latitude limits.\n",
    "    tpickupnorth = tpickup_latitude <= northlimit \n",
    "    tpickupsouth = tpickup_latitude >= southlimit\n",
    "    tpickuplat = tpickupnorth * tpickupsouth \n",
    "    \n",
    "    # Create final pickup filter.\n",
    "    tpickupfilter = tpickuplon * tpickuplat\n",
    "        \n",
    "    # Filter out dropoff data.\n",
    "    tdropoff_longitude = dataset['dropoff_longitude'].values\n",
    "    tdropoff_latitude = dataset['dropoff_latitude'].values\n",
    "        \n",
    "    # Create filters for data within longitude limits.\n",
    "    tdropoffwest = tdropoff_longitude >= westlimit\n",
    "    tdropoffeast = tdropoff_longitude <= eastlimit \n",
    "    tdropofflon = tdropoffwest * tdropoffeast \n",
    "    \n",
    "    # Create filters for data within latitude limits.\n",
    "    tdropoffnorth = tdropoff_latitude <= northlimit \n",
    "    tdropoffsouth = tdropoff_latitude >= southlimit\n",
    "    tdropofflat = tdropoffnorth * tdropoffsouth \n",
    "    \n",
    "    # Create final dropoff filter.\n",
    "    tdropofffilter = tdropofflon * tdropofflat\n",
    "    \n",
    "    # Define final boundary filter.\n",
    "    tfinalfilter = tpickupfilter * tdropofffilter\n",
    "    \n",
    "    # Apply the final filter to the dataframe.\n",
    "    dataset = dataset[tfinalfilter]\n",
    "    \n",
    "    # Create a random sample of data to match the size of the Uber dataset (2380 rows from each taxi dataset).\n",
    "    dataset = dataset.sample(2380)\n",
    "    \n",
    "    # Normalize column datatypes.\n",
    "    dataset[\"pickup_datetime\"]=pd.to_datetime(dataset[\"pickup_datetime\"])\n",
    "\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d2824fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_month_taxi_data2(url):\n",
    "    \"\"\"Filter and clean the taxi data for analysis purposes.\"\"\"\n",
    "\n",
    "    # Use regex to math to desired format of the datafiles.\n",
    "    pattern1 = \"yellow_tripdata_20\\d\\d-\\d\\d.parquet\"\n",
    "    if re.search(pattern1, url) is not None: \n",
    "        filename = re.search(pattern1, url).group()\n",
    "    \n",
    "    dataset = pd.read_parquet(filename, engine='fastparquet')\n",
    "    \n",
    "    dataset = dataset[[\"pickup_datetime\", \"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\"]]\n",
    "    \n",
    "    # Filter and remove rows with invalid coordinates.\n",
    "    dataset = dataset[~(dataset[['pickup_longitude','pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']] == 0).any(axis=1)] \n",
    "    \n",
    "    # Define longitude and latitude bounds.\n",
    "    westlimit = -74.242330\n",
    "    eastlimit = -73.717047\n",
    "\n",
    "    southlimit = 40.560445\n",
    "    northlimit = 40.908524\n",
    "    \n",
    "    # Filter out pickup data.\n",
    "    tpickup_longitude = dataset['pickup_longitude'].values\n",
    "    tpickup_latitude = dataset['pickup_latitude'].values\n",
    "    \n",
    "    # Create filters for data within longitude limits.\n",
    "    tpickupwest = tpickup_longitude >= westlimit\n",
    "    tpickupeast = tpickup_longitude <= eastlimit \n",
    "    tpickuplon = tpickupwest * tpickupeast \n",
    "    \n",
    "    # Create filters for data within latitude limits. \n",
    "    tpickupnorth = tpickup_latitude <= northlimit \n",
    "    tpickupsouth = tpickup_latitude >= southlimit\n",
    "    tpickuplat = tpickupnorth * tpickupsouth \n",
    "    \n",
    "    # Create final pickup filter.\n",
    "    tpickupfilter = tpickuplon * tpickuplat\n",
    "    \n",
    "    # filtering out dropoff data\n",
    "    tdropoff_longitude = dataset['dropoff_longitude'].values\n",
    "    tdropoff_latitude = dataset['dropoff_latitude'].values\n",
    "        \n",
    "    # Create filters for data within longitude limits.\n",
    "    tdropoffwest = tdropoff_longitude >= westlimit\n",
    "    tdropoffeast = tdropoff_longitude <= eastlimit \n",
    "    tdropofflon = tdropoffwest * tdropoffeast \n",
    "    \n",
    "    # Create filters for data within latitude limits. \n",
    "    tdropoffnorth = tdropoff_latitude <= northlimit \n",
    "    tdropoffsouth = tdropoff_latitude >= southlimit\n",
    "    tdropofflat = tdropoffnorth * tdropoffsouth \n",
    "    \n",
    "    # Create final dropoff filter.\n",
    "    tdropofffilter = tdropofflon * tdropofflat\n",
    "    \n",
    "    # Create final boundary filter.\n",
    "    tfinalfilter = tpickupfilter * tdropofffilter\n",
    "    \n",
    "    # Apply final filter to dataframe.\n",
    "    dataset = dataset[tfinalfilter]\n",
    "    \n",
    "    # Create a random sample of data to match the size of the Uber dataset (2380 rows from each taxi dataset).\n",
    "    dataset = dataset.sample(2380)\n",
    "    \n",
    "    # Normalize column datatypes.\n",
    "    dataset[\"pickup_datetime\"]=pd.to_datetime(dataset[\"pickup_datetime\"])\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ed18b5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_month_taxi_data3(url):\n",
    "    \"\"\"Filter and clean the taxi data for analysis purposes.\"\"\"\n",
    "\n",
    "    # Import shapefile.\n",
    "    shapefile = gpd.read_file(TAXI_SHPFILE)\n",
    "    \n",
    "    # Use imported shapefile to convert location IDs to coordinate values.\n",
    "    shapefile = shapefile[[\"LocationID\",\"geometry\"]]\n",
    "    shapefile = shapefile.to_crs(epsg = 4326)\n",
    "    \n",
    "    # Create new longitude and latitude columns.\n",
    "    shapefile[\"lon\"] = shapefile.geometry.centroid.x\n",
    "    shapefile[\"lat\"] = shapefile.geometry.centroid.y\n",
    "    shapefile.index = np.arange(1,264)\n",
    "    shapefile = shapefile[['lon','lat']]\n",
    "    longitude = shapefile['lon']\n",
    "    latitude = shapefile['lat']\n",
    "    \n",
    "    # Use regex to math to desired format of the datafiles.\n",
    "    pattern = \"yellow_tripdata_20\\d\\d-\\d\\d.parquet\"\n",
    "    if re.search(pattern, url) is not None: \n",
    "        filename = re.search(pattern, url).group()\n",
    "    \n",
    "    dataset = pd.read_parquet(filename, engine='fastparquet')\n",
    "    dataset = dataset[[\"tpep_pickup_datetime\", \"PULocationID\", \"DOLocationID\", \"trip_distance\"]]\n",
    "    dataset = dataset.set_axis([\"pickup_datetime\", \"PULocationID\", \"DOLocationID\", \"distance\"], axis=1, inplace=False)\n",
    "    \n",
    "    # Create boolean masks for valid Location IDs.\n",
    "    pulocationidfilter = dataset['PULocationID'] <= 263\n",
    "    dolocationidfilter = dataset['DOLocationID'] <= 263\n",
    "    validityfilter = pulocationidfilter & dolocationidfilter\n",
    "    \n",
    "    dataset = dataset[validityfilter] # Get rid  of nonvalid IDs. \n",
    "\n",
    "    pulon = []\n",
    "    pulat = []\n",
    "    for idpu in dataset['PULocationID']: \n",
    "        pulon.append(longitude[idpu])\n",
    "        pulat.append(latitude[idpu])\n",
    "        \n",
    "    dataset['pickup_longitude'] = pulon\n",
    "    dataset['pickup_latitude'] = pulat\n",
    "        \n",
    "    dolon = []\n",
    "    dolat = []\n",
    "    for iddo in dataset['DOLocationID']: \n",
    "        dolon.append(longitude[iddo])\n",
    "        dolat.append(latitude[iddo])\n",
    "        \n",
    "    dataset['dropoff_longitude'] = dolon\n",
    "    dataset['dropoff_latitude'] = dolat\n",
    "    \n",
    "    dataset = dataset[[\"pickup_datetime\", \"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\"]]\n",
    "    \n",
    "    # Filter rows with invalid coordinates.\n",
    "    dataset = dataset[~(dataset[['pickup_longitude','pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']] == 0).any(axis=1)] \n",
    "    \n",
    "    # Define longitude and latitude bounds.\n",
    "    westlimit = -74.242330\n",
    "    eastlimit = -73.717047\n",
    "     \n",
    "    southlimit = 40.560445\n",
    "    northlimit = 40.908524\n",
    "    \n",
    "    # Filter out pickup data. \n",
    "    tpickup_longitude = dataset['pickup_longitude'].values\n",
    "    tpickup_latitude = dataset['pickup_latitude'].values\n",
    "    \n",
    "    # Create filters for data within longitude limits.\n",
    "    tpickupwest = tpickup_longitude >= westlimit\n",
    "    tpickupeast = tpickup_longitude <= eastlimit \n",
    "    tpickuplon = tpickupwest * tpickupeast \n",
    "    \n",
    "    # Create filters for data within latitude limits.\n",
    "    tpickupnorth = tpickup_latitude <= northlimit \n",
    "    tpickupsouth = tpickup_latitude >= southlimit\n",
    "    tpickuplat = tpickupnorth * tpickupsouth \n",
    "    \n",
    "    # Create final pickup filter.\n",
    "    tpickupfilter = tpickuplon * tpickuplat \n",
    "\n",
    "    # Filter out dropoff data.\n",
    "    tdropoff_longitude = dataset['dropoff_longitude'].values\n",
    "    tdropoff_latitude = dataset['dropoff_latitude'].values\n",
    "        \n",
    "    # Create filters for data within longitude limits. \n",
    "    tdropoffwest = tdropoff_longitude >= westlimit\n",
    "    tdropoffeast = tdropoff_longitude <= eastlimit \n",
    "    tdropofflon = tdropoffwest * tdropoffeast \n",
    "    \n",
    "    # Create filters for data within latitude limits. \n",
    "    tdropoffnorth = tdropoff_latitude <= northlimit \n",
    "    tdropoffsouth = tdropoff_latitude >= southlimit\n",
    "    tdropofflat = tdropoffnorth * tdropoffsouth \n",
    "    \n",
    "    # Create final dropoff filter. \n",
    "    tdropofffilter = tdropofflon * tdropofflat\n",
    "    \n",
    "    # Create final boundary filter.\n",
    "    tfinalfilter = tpickupfilter * tdropofffilter\n",
    "    \n",
    "    # Apply final filter to dataframe.\n",
    "    dataset = dataset[tfinalfilter]\n",
    "    \n",
    "    # Create a random sample of data to match the size of the Uber dataset (2380 rows from each taxi dataset).\n",
    "    dataset = dataset.sample(2380)\n",
    "    \n",
    "    # Normalize column datatypes.\n",
    "    dataset[\"pickup_datetime\"]=pd.to_datetime(dataset[\"pickup_datetime\"])\n",
    "              \n",
    "    return dataset\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "35c9c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_taxi_data():\n",
    "    \"\"\"Apply all created filters and concatinate into one dataframe.\"\"\"\n",
    "    all_taxi_dataframes = []\n",
    "    taxi_urls = find_taxi_csv_urls()\n",
    "    \n",
    "    # Split urls. \n",
    "    csv_urls1 = taxi_urls[:12]\n",
    "    csv_urls2 = taxi_urls[12:24]\n",
    "    csv_urls3 = taxi_urls[24:]\n",
    "\n",
    "    # Apply preprocessing functions to applicable dataframes.\n",
    "    for csv_url in csv_urls1:\n",
    "        dataframe1 = get_and_clean_month_taxi_data1(csv_url)\n",
    "        add_distance_column(dataframe1)\n",
    "        all_taxi_dataframes.append(dataframe1)\n",
    "    for csv_url in csv_urls2:\n",
    "        dataframe2 = get_and_clean_month_taxi_data2(csv_url)\n",
    "        add_distance_column(dataframe2)\n",
    "        all_taxi_dataframes.append(dataframe2)\n",
    "    for csv_url in csv_urls3:\n",
    "        dataframe3 = get_and_clean_month_taxi_data3(csv_url)\n",
    "        add_distance_column(dataframe3)\n",
    "        all_taxi_dataframes.append(dataframe3)\n",
    "        \n",
    "    # Create one dataframe with data from every month needed.\n",
    "    taxi_data = []\n",
    "    for dataframe in all_taxi_dataframes: \n",
    "        taxi_data.append(dataframe)\n",
    "        \n",
    "                            \n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a15cbb",
   "metadata": {},
   "source": [
    "### Processing Weather Data\n",
    "\n",
    "In this section, the processesing of data before performing analyses continues. The data is split into two different dataframes so that alayses can be run at different frequencies. NaN and invalid data points are dropped as part of cleaning the data. Regex is also used to extract just didgits from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "76e864ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_hourly(csv_file):\n",
    "    \"\"\"Filter and clean the weather data for analysis.\"\"\"\n",
    "    \n",
    "    # Import/read csv file.\n",
    "    data = pd.read_csv(csv_file, low_memory = False) \n",
    "    \n",
    "    # Clean dataframe by dropping NAN values.\n",
    "    cleaned_data = data.dropna(axis=0, subset = \"HourlyPrecipitation\") \n",
    "    \n",
    "    # Remove rows with trace ('T') values.\n",
    "    cleaned_data = cleaned_data[~(cleaned_data[\"HourlyPrecipitation\"] == 'T')]\n",
    "    \n",
    "    # Only extract digits from Hourly Precipitation column. \n",
    "    cleaned_data[\"HourlyPrecipitation\"] = cleaned_data[\"HourlyPrecipitation\"].replace('\\(|[a-zA-Z]+', '', regex=True)\n",
    "    \n",
    "    # Drop NAN values for wind speed.\n",
    "    cleaned_data = cleaned_data.dropna(axis=0, subset = \"HourlyWindSpeed\") \n",
    "    \n",
    "    # Create a dataframe with only the necessary columns. \n",
    "    cleaned_data = cleaned_data[[\"DATE\",\"HourlyPrecipitation\",\"HourlyWindSpeed\"]]\n",
    "    \n",
    "    # Standardize datatypes.\n",
    "    cleaned_data[\"DATE\"] = pd.to_datetime(cleaned_data[\"DATE\"])\n",
    "    cleaned_data[\"HourlyPrecipitation\"] = pd.to_numeric(cleaned_data[\"HourlyPrecipitation\"])\n",
    "    \n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0687581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_daily(csv_file):\n",
    "    \"\"\"Create Daily Weather dataframe\"\"\"\n",
    "    \n",
    "    # Load cleaned hourly data.\n",
    "    cleaned_hourly = clean_month_weather_data_hourly(csv_file)\n",
    "    \n",
    "    # Group by each date and assign the average of the hourly values to a daily value.\n",
    "    daily = cleaned_hourly.groupby([cleaned_hourly['DATE'].dt.date]).mean()\n",
    "    \n",
    "    return daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3ef8945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_weather_data():\n",
    "    \"\"\"Generate the final weather dataframes.\"\"\"\n",
    "    hourly_dataframes = []\n",
    "    daily_dataframes = []\n",
    "    \n",
    "    # Add the name/paths.\n",
    "    weather_csv_files = WEATHER_DATA_FILES\n",
    "    \n",
    "    for csv_file in weather_csv_files:\n",
    "        hourly_dataframe = clean_month_weather_data_hourly(csv_file)\n",
    "        daily_dataframe = clean_month_weather_data_daily(csv_file)\n",
    "        hourly_dataframes.append(hourly_dataframe)\n",
    "        daily_dataframes.append(daily_dataframe)\n",
    "        \n",
    "    # Create two dataframes with hourly & daily data from every source.\n",
    "    hourly_data = pd.concat(hourly_dataframes)\n",
    "    daily_data = pd.concat(daily_dataframes)\n",
    "    \n",
    "    return hourly_data, daily_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f900f7aa",
   "metadata": {},
   "source": [
    "### Process All Data\n",
    "\n",
    "After writing the code for cleaning and preprocessing all of the data, the below cells are meant to execute all of the required functions and by default make sure they all run properly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f7cd53a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johnf\\AppData\\Local\\Temp\\ipykernel_12392\\2053059540.py:12: FutureWarning: DataFrame.set_axis 'inplace' keyword is deprecated and will be removed in a future version. Use `obj = obj.set_axis(..., copy=False)` instead\n",
      "  dataset = dataset.set_axis([\"pickup_datetime\", \"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\"], axis=1, inplace=False)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12392\\147857205.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtaxi_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_and_clean_taxi_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12392\\3629822002.py\u001b[0m in \u001b[0;36mget_and_clean_taxi_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m# Apply preprocessing functions to applicable dataframes.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcsv_url\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcsv_urls1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mdataframe1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_and_clean_month_taxi_data1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0madd_distance_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataframe1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mall_taxi_dataframes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataframe1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12392\\2053059540.py\u001b[0m in \u001b[0;36mget_and_clean_month_taxi_data1\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'fastparquet'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Trip_Pickup_DateTime\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Start_Lon\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Start_Lat\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"End_Lon\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"End_Lat\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parquet.py\u001b[0m in \u001b[0;36mread_parquet\u001b[1;34m(path, engine, columns, storage_options, use_nullable_dtypes, **kwargs)\u001b[0m\n\u001b[0;32m    501\u001b[0m     \u001b[0mimpl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    502\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 503\u001b[1;33m     return impl.read(\n\u001b[0m\u001b[0;32m    504\u001b[0m         \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m         \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parquet.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, path, columns, storage_options, **kwargs)\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m             \u001b[0mparquet_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mParquetFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mparquet_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 358\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mparquet_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_pandas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    359\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\fastparquet\\api.py\u001b[0m in \u001b[0;36mto_pandas\u001b[1;34m(self, columns, categories, filters, index, row_filter, dtypes)\u001b[0m\n\u001b[0;32m    774\u001b[0m                             else v[start:start + thislen])\n\u001b[0;32m    775\u001b[0m                      for (name, v) in views.items()}\n\u001b[1;32m--> 776\u001b[1;33m             self.read_row_group_file(rg, columns, categories, index,\n\u001b[0m\u001b[0;32m    777\u001b[0m                                      \u001b[0massign\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartition_meta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartition_meta\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m                                      row_filter=sel, infile=infile)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\fastparquet\\api.py\u001b[0m in \u001b[0;36mread_row_group_file\u001b[1;34m(self, rg, columns, categories, index, assign, partition_meta, row_filter, infile)\u001b[0m\n\u001b[0;32m    378\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minfile\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 380\u001b[1;33m         core.read_row_group(\n\u001b[0m\u001b[0;32m    381\u001b[0m             \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategories\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcats\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m             \u001b[0mselfmade\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselfmade\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\fastparquet\\core.py\u001b[0m in \u001b[0;36mread_row_group\u001b[1;34m(file, rg, columns, categories, schema_helper, cats, selfmade, index, assign, scheme, partition_meta, row_filter)\u001b[0m\n\u001b[0;32m    616\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0massign\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Going with pre-allocation!'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 618\u001b[1;33m     read_row_group_arrays(file, rg, columns, categories, schema_helper,\n\u001b[0m\u001b[0;32m    619\u001b[0m                           cats, selfmade, assign=assign, row_filter=row_filter)\n\u001b[0;32m    620\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\fastparquet\\core.py\u001b[0m in \u001b[0;36mread_row_group_arrays\u001b[1;34m(file, rg, columns, categories, schema_helper, cats, selfmade, assign, row_filter)\u001b[0m\n\u001b[0;32m    586\u001b[0m         \u001b[0mremains\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiscard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 588\u001b[1;33m         read_col(column, schema_helper, file, use_cat=name+'-catdef' in out,\n\u001b[0m\u001b[0;32m    589\u001b[0m                  \u001b[0mselfmade\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mselfmade\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0massign\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m                  \u001b[0mcatdef\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'-catdef'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\fastparquet\\core.py\u001b[0m in \u001b[0;36mread_col\u001b[1;34m(column, schema_helper, infile, use_cat, selfmade, assign, catdef, row_filter)\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mskip_nulls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 492\u001b[1;33m         defi, rep, val = read_data_page(infile, schema_helper, ph, cmd,\n\u001b[0m\u001b[0;32m    493\u001b[0m                                         skip_nulls, selfmade=selfmade)\n\u001b[0;32m    494\u001b[0m         \u001b[0mmax_defi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mschema_helper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_definition_level\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath_in_schema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\fastparquet\\core.py\u001b[0m in \u001b[0;36mread_data_page\u001b[1;34m(f, helper, header, metadata, skip_nulls, selfmade)\u001b[0m\n\u001b[0;32m    112\u001b[0m     \"\"\"\n\u001b[0;32m    113\u001b[0m     \u001b[0mdaph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_page_header\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m     \u001b[0mraw_bytes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_read_page\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m     \u001b[0mio_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNumpyIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_bytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\fastparquet\\core.py\u001b[0m in \u001b[0;36m_read_page\u001b[1;34m(file_obj, page_header, column_metadata)\u001b[0m\n\u001b[0;32m     19\u001b[0m     uncompressed bytes (if necessary).\"\"\"\n\u001b[0;32m     20\u001b[0m     \u001b[0mraw_bytes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage_header\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompressed_page_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     raw_bytes = decompress_data(\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0mraw_bytes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mpage_header\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muncompressed_page_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "taxi_data = get_and_clean_taxi_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c7c29c98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6891767</th>\n",
       "      <td>2009-01-08 22:35:00</td>\n",
       "      <td>-73.981912</td>\n",
       "      <td>40.773760</td>\n",
       "      <td>-73.932862</td>\n",
       "      <td>40.797493</td>\n",
       "      <td>5.504381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14030472</th>\n",
       "      <td>2009-01-16 10:22:30</td>\n",
       "      <td>-73.955361</td>\n",
       "      <td>40.768600</td>\n",
       "      <td>-73.953716</td>\n",
       "      <td>40.764845</td>\n",
       "      <td>0.216348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10841276</th>\n",
       "      <td>2009-01-19 17:00:00</td>\n",
       "      <td>-73.953937</td>\n",
       "      <td>40.766288</td>\n",
       "      <td>-73.970498</td>\n",
       "      <td>40.788372</td>\n",
       "      <td>1.963107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10888292</th>\n",
       "      <td>2009-01-22 23:34:00</td>\n",
       "      <td>-74.000023</td>\n",
       "      <td>40.732717</td>\n",
       "      <td>-73.986537</td>\n",
       "      <td>40.732678</td>\n",
       "      <td>1.500046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3420072</th>\n",
       "      <td>2009-01-31 13:32:00</td>\n",
       "      <td>-73.966833</td>\n",
       "      <td>40.788605</td>\n",
       "      <td>-73.967702</td>\n",
       "      <td>40.761842</td>\n",
       "      <td>0.827826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9038939</th>\n",
       "      <td>2015-06-22 19:13:16</td>\n",
       "      <td>-73.978492</td>\n",
       "      <td>40.747746</td>\n",
       "      <td>-73.965635</td>\n",
       "      <td>40.768615</td>\n",
       "      <td>1.567144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179167</th>\n",
       "      <td>2015-06-01 13:11:57</td>\n",
       "      <td>-73.976495</td>\n",
       "      <td>40.740439</td>\n",
       "      <td>-73.984052</td>\n",
       "      <td>40.736824</td>\n",
       "      <td>0.847901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2894518</th>\n",
       "      <td>2015-06-07 18:49:26</td>\n",
       "      <td>-73.990458</td>\n",
       "      <td>40.740337</td>\n",
       "      <td>-73.981414</td>\n",
       "      <td>40.670374</td>\n",
       "      <td>2.370846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11367055</th>\n",
       "      <td>2015-06-28 10:34:30</td>\n",
       "      <td>-73.965635</td>\n",
       "      <td>40.768615</td>\n",
       "      <td>-73.951010</td>\n",
       "      <td>40.778766</td>\n",
       "      <td>1.656349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11954257</th>\n",
       "      <td>2015-06-30 00:51:53</td>\n",
       "      <td>-73.984052</td>\n",
       "      <td>40.736824</td>\n",
       "      <td>-73.992438</td>\n",
       "      <td>40.748497</td>\n",
       "      <td>0.999104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>185640 rows  6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             pickup_datetime  pickup_longitude  pickup_latitude  \\\n",
       "6891767  2009-01-08 22:35:00        -73.981912        40.773760   \n",
       "14030472 2009-01-16 10:22:30        -73.955361        40.768600   \n",
       "10841276 2009-01-19 17:00:00        -73.953937        40.766288   \n",
       "10888292 2009-01-22 23:34:00        -74.000023        40.732717   \n",
       "3420072  2009-01-31 13:32:00        -73.966833        40.788605   \n",
       "...                      ...               ...              ...   \n",
       "9038939  2015-06-22 19:13:16        -73.978492        40.747746   \n",
       "179167   2015-06-01 13:11:57        -73.976495        40.740439   \n",
       "2894518  2015-06-07 18:49:26        -73.990458        40.740337   \n",
       "11367055 2015-06-28 10:34:30        -73.965635        40.768615   \n",
       "11954257 2015-06-30 00:51:53        -73.984052        40.736824   \n",
       "\n",
       "          dropoff_longitude  dropoff_latitude  distance  \n",
       "6891767          -73.932862         40.797493  5.504381  \n",
       "14030472         -73.953716         40.764845  0.216348  \n",
       "10841276         -73.970498         40.788372  1.963107  \n",
       "10888292         -73.986537         40.732678  1.500046  \n",
       "3420072          -73.967702         40.761842  0.827826  \n",
       "...                     ...               ...       ...  \n",
       "9038939          -73.965635         40.768615  1.567144  \n",
       "179167           -73.984052         40.736824  0.847901  \n",
       "2894518          -73.981414         40.670374  2.370846  \n",
       "11367055         -73.951010         40.778766  1.656349  \n",
       "11954257         -73.992438         40.748497  0.999104  \n",
       "\n",
       "[185640 rows x 6 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_data = pd.concat(taxi_data)\n",
    "taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "61b49fcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-05-07 19:52:06+00:00</td>\n",
       "      <td>-73.999817</td>\n",
       "      <td>40.738354</td>\n",
       "      <td>-73.999512</td>\n",
       "      <td>40.723217</td>\n",
       "      <td>0.465327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-07-17 20:04:56+00:00</td>\n",
       "      <td>-73.994355</td>\n",
       "      <td>40.728225</td>\n",
       "      <td>-73.994710</td>\n",
       "      <td>40.750325</td>\n",
       "      <td>0.678941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-08-24 21:45:00+00:00</td>\n",
       "      <td>-74.005043</td>\n",
       "      <td>40.740770</td>\n",
       "      <td>-73.962565</td>\n",
       "      <td>40.772647</td>\n",
       "      <td>4.825036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-06-26 08:22:21+00:00</td>\n",
       "      <td>-73.976124</td>\n",
       "      <td>40.790844</td>\n",
       "      <td>-73.965316</td>\n",
       "      <td>40.803349</td>\n",
       "      <td>1.262035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-08-28 17:47:00+00:00</td>\n",
       "      <td>-73.925023</td>\n",
       "      <td>40.744085</td>\n",
       "      <td>-73.973082</td>\n",
       "      <td>40.761247</td>\n",
       "      <td>5.371588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>2012-10-28 10:49:00+00:00</td>\n",
       "      <td>-73.987042</td>\n",
       "      <td>40.739367</td>\n",
       "      <td>-73.986525</td>\n",
       "      <td>40.740297</td>\n",
       "      <td>0.064197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>2014-03-14 01:09:00+00:00</td>\n",
       "      <td>-73.984722</td>\n",
       "      <td>40.736837</td>\n",
       "      <td>-74.006672</td>\n",
       "      <td>40.739620</td>\n",
       "      <td>2.442986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>2009-06-29 00:42:00+00:00</td>\n",
       "      <td>-73.986017</td>\n",
       "      <td>40.756487</td>\n",
       "      <td>-73.858957</td>\n",
       "      <td>40.692588</td>\n",
       "      <td>14.269270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>2015-05-20 14:56:25+00:00</td>\n",
       "      <td>-73.997124</td>\n",
       "      <td>40.725452</td>\n",
       "      <td>-73.983215</td>\n",
       "      <td>40.695415</td>\n",
       "      <td>1.800660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199999</th>\n",
       "      <td>2010-05-15 04:08:00+00:00</td>\n",
       "      <td>-73.984395</td>\n",
       "      <td>40.720077</td>\n",
       "      <td>-73.985508</td>\n",
       "      <td>40.768793</td>\n",
       "      <td>1.500074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>195472 rows  6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 pickup_datetime  pickup_longitude  pickup_latitude  \\\n",
       "0      2015-05-07 19:52:06+00:00        -73.999817        40.738354   \n",
       "1      2009-07-17 20:04:56+00:00        -73.994355        40.728225   \n",
       "2      2009-08-24 21:45:00+00:00        -74.005043        40.740770   \n",
       "3      2009-06-26 08:22:21+00:00        -73.976124        40.790844   \n",
       "4      2014-08-28 17:47:00+00:00        -73.925023        40.744085   \n",
       "...                          ...               ...              ...   \n",
       "199995 2012-10-28 10:49:00+00:00        -73.987042        40.739367   \n",
       "199996 2014-03-14 01:09:00+00:00        -73.984722        40.736837   \n",
       "199997 2009-06-29 00:42:00+00:00        -73.986017        40.756487   \n",
       "199998 2015-05-20 14:56:25+00:00        -73.997124        40.725452   \n",
       "199999 2010-05-15 04:08:00+00:00        -73.984395        40.720077   \n",
       "\n",
       "        dropoff_longitude  dropoff_latitude   distance  \n",
       "0              -73.999512         40.723217   0.465327  \n",
       "1              -73.994710         40.750325   0.678941  \n",
       "2              -73.962565         40.772647   4.825036  \n",
       "3              -73.965316         40.803349   1.262035  \n",
       "4              -73.973082         40.761247   5.371588  \n",
       "...                   ...               ...        ...  \n",
       "199995         -73.986525         40.740297   0.064197  \n",
       "199996         -74.006672         40.739620   2.442986  \n",
       "199997         -73.858957         40.692588  14.269270  \n",
       "199998         -73.983215         40.695415   1.800660  \n",
       "199999         -73.985508         40.768793   1.500074  \n",
       "\n",
       "[195472 rows x 6 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uber_data = get_uber_data()\n",
    "uber_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a502ce40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johnf\\AppData\\Local\\Temp\\ipykernel_20196\\1294668373.py:4: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  daily = cleaned_hourly.groupby([cleaned_hourly['DATE'].dt.date]).mean()\n",
      "C:\\Users\\johnf\\AppData\\Local\\Temp\\ipykernel_20196\\1294668373.py:4: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  daily = cleaned_hourly.groupby([cleaned_hourly['DATE'].dt.date]).mean()\n",
      "C:\\Users\\johnf\\AppData\\Local\\Temp\\ipykernel_20196\\1294668373.py:4: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  daily = cleaned_hourly.groupby([cleaned_hourly['DATE'].dt.date]).mean()\n",
      "C:\\Users\\johnf\\AppData\\Local\\Temp\\ipykernel_20196\\1294668373.py:4: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  daily = cleaned_hourly.groupby([cleaned_hourly['DATE'].dt.date]).mean()\n",
      "C:\\Users\\johnf\\AppData\\Local\\Temp\\ipykernel_20196\\1294668373.py:4: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  daily = cleaned_hourly.groupby([cleaned_hourly['DATE'].dt.date]).mean()\n",
      "C:\\Users\\johnf\\AppData\\Local\\Temp\\ipykernel_20196\\1294668373.py:4: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  daily = cleaned_hourly.groupby([cleaned_hourly['DATE'].dt.date]).mean()\n",
      "C:\\Users\\johnf\\AppData\\Local\\Temp\\ipykernel_20196\\1294668373.py:4: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  daily = cleaned_hourly.groupby([cleaned_hourly['DATE'].dt.date]).mean()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DailyPrecipitation</th>\n",
       "      <th>DailyWindSpeed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATE</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2009-01-06</th>\n",
       "      <td>0.017143</td>\n",
       "      <td>8.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-01-07</th>\n",
       "      <td>0.058710</td>\n",
       "      <td>10.387097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-01-10</th>\n",
       "      <td>0.020500</td>\n",
       "      <td>9.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-01-11</th>\n",
       "      <td>0.039231</td>\n",
       "      <td>8.769231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-01-15</th>\n",
       "      <td>0.010000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-27</th>\n",
       "      <td>0.007391</td>\n",
       "      <td>5.521739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-28</th>\n",
       "      <td>0.001500</td>\n",
       "      <td>8.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-29</th>\n",
       "      <td>0.028182</td>\n",
       "      <td>7.303030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-30</th>\n",
       "      <td>0.011154</td>\n",
       "      <td>4.115385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-31</th>\n",
       "      <td>0.003333</td>\n",
       "      <td>5.375000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1664 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            DailyPrecipitation  DailyWindSpeed\n",
       "DATE                                          \n",
       "2009-01-06            0.017143        8.857143\n",
       "2009-01-07            0.058710       10.387097\n",
       "2009-01-10            0.020500        9.250000\n",
       "2009-01-11            0.039231        8.769231\n",
       "2009-01-15            0.010000        7.000000\n",
       "...                        ...             ...\n",
       "2015-12-27            0.007391        5.521739\n",
       "2015-12-28            0.001500        8.150000\n",
       "2015-12-29            0.028182        7.303030\n",
       "2015-12-30            0.011154        4.115385\n",
       "2015-12-31            0.003333        5.375000\n",
       "\n",
       "[1664 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hourly_weather_data, daily_weather_data = load_and_clean_weather_data()\n",
    "daily_weather_data = daily_weather_data.rename(columns={'HourlyPrecipitation':'DailyPrecipitation',\n",
    "                                  'HourlyWindSpeed':'DailyWindSpeed'})\n",
    "daily_weather_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd101f11",
   "metadata": {},
   "source": [
    "## Part 2: Storing Cleaned Data\n",
    "\n",
    "While the dataframes we have stored our data in are convenient for coding purposes, they are impermanent and take a long time to process, so we would like to store them so that we do not have to generate them every time. We do this by writing them into SQL tables in a database we define. We will name our database \"project.db\" and write our dataframes into it by using Pandas' .to_sql functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3529cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = db.create_engine('sqlite:///project.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a776b4a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "185640"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hourly_weather_data.to_sql('hourlyWeather',con=engine, if_exists='replace',index_label='id')\n",
    "daily_weather_data.to_sql('dailyWeather',con=engine, if_exists='replace',index_label='id')\n",
    "uber_data.to_sql('uber',con=engine, if_exists='replace',index_label='id')\n",
    "taxi_data.to_sql('taxi',con=engine, if_exists='replace',index_label='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb6e33e",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data\n",
    "\n",
    "In this section, we will run several queries on the data we have stored in SQL tables. We do this to get a better understanding of the shape and scope of our data. For each query, we have a particular question in mind that we will answer by calling relevant data. At times, we may summarize the data by counting rows or taking averages. In the next cell, we define a function that will write the prose of our queries into .sql files so that we can call them later without having to write them again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6a849e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_query_to_file(query, outfile):\n",
    "    \"\"\"Write a given SQL query string to a file called outfile.\"\"\"\n",
    "    f = open(outfile,'w')\n",
    "    f.write(query)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee70a777",
   "metadata": {},
   "source": [
    "### Query 1\n",
    "\n",
    "Here we will find what hour of the day was the most popular to take a Yellow Taxi by counting the number of rows in our table corresponding to each hour of the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "db871d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_1 = \"\"\"\n",
    "SELECT \n",
    "COUNT(CASE WHEN pickup_datetime LIKE '%00:%' THEN 1 END) AS '00',\n",
    "COUNT(CASE WHEN pickup_datetime LIKE '%01:%' THEN 1 END) AS '01',\n",
    "COUNT(CASE WHEN pickup_datetime LIKE '%02:%' THEN 1 END) AS '02',\n",
    "COUNT(CASE WHEN pickup_datetime LIKE '%03:%' THEN 1 END) AS '03',\n",
    "COUNT(CASE WHEN pickup_datetime LIKE '%04:%' THEN 1 END) AS '04',\n",
    "COUNT(CASE WHEN pickup_datetime LIKE '%05:%' THEN 1 END) AS '05',\n",
    "COUNT(CASE WHEN pickup_datetime LIKE '%06:%' THEN 1 END) AS '06',\n",
    "COUNT(CASE WHEN pickup_datetime LIKE '%07:%' THEN 1 END) AS '07',\n",
    "COUNT(CASE WHEN pickup_datetime LIKE '%08:%' THEN 1 END) AS '08',\n",
    "COUNT(CASE WHEN pickup_datetime LIKE '%09:%' THEN 1 END) AS '09',\n",
    "COUNT(CASE WHEN pickup_datetime LIKE '%10:%' THEN 1 END) AS '10',\n",
    "COUNT(CASE WHEN pickup_datetime LIKE '%11:%' THEN 1 END) AS '11',\n",
    "COUNT(CASE WHEN pickup_datetime LIKE '%12:%' THEN 1 END) AS '12',\n",
    "COUNT(CASE WHEN pickup_datetime LIKE '%13:%' THEN 1 END) AS '13',\n",
    "COUNT(CASE WHEN pickup_datetime LIKE '%14:%' THEN 1 END) AS '14',\n",
    "COUNT(CASE WHEN pickup_datetime LIKE '%15:%' THEN 1 END) AS '15',\n",
    "COUNT(CASE WHEN pickup_datetime LIKE '%16:%' THEN 1 END) AS '16',\n",
    "COUNT(CASE WHEN pickup_datetime LIKE '%17:%' THEN 1 END) AS '17',\n",
    "COUNT(CASE WHEN pickup_datetime LIKE '%18:%' THEN 1 END) AS '18',\n",
    "COUNT(CASE WHEN pickup_datetime LIKE '%19:%' THEN 1 END) AS '19',\n",
    "COUNT(CASE WHEN pickup_datetime LIKE '%20:%' THEN 1 END) AS '20',\n",
    "COUNT(CASE WHEN pickup_datetime LIKE '%21:%' THEN 1 END) AS '21',\n",
    "COUNT(CASE WHEN pickup_datetime LIKE '%22:%' THEN 1 END) AS '22',\n",
    "COUNT(CASE WHEN pickup_datetime LIKE '%23:%' THEN 1 END) AS '23'\n",
    "FROM taxi\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c5275f3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(10198, 8420, 7064, 5899, 5128, 4950, 6710, 9844, 11445, 11797, 11223, 11812, 12021, 12016, 12171, 11885, 10501, 12413, 13956, 14770, 13902, 13585, 13149, 11934)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.execute(QUERY_1).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a2ef04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_1, \"popular_taxi_hours.sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898e78ad",
   "metadata": {},
   "source": [
    "### Query 2\n",
    "\n",
    "Here we will find what day was the most popular to take an Uber by counting the number of rows in our table corresponding to each day of the week.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "400124c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_2 = \"\"\"\n",
    "SELECT\n",
    "COUNT(CASE WHEN strftime('%w',pickup_datetime) IS '0' THEN 1 END) AS 'Sunday',\n",
    "COUNT(CASE WHEN strftime('%w',pickup_datetime) IS '1' THEN 1 END) AS 'Monday',\n",
    "COUNT(CASE WHEN strftime('%w',pickup_datetime) IS '2' THEN 1 END) AS 'Tuesday',\n",
    "COUNT(CASE WHEN strftime('%w',pickup_datetime) IS '3' THEN 1 END) AS 'Wednesday',\n",
    "COUNT(CASE WHEN strftime('%w',pickup_datetime) IS '4' THEN 1 END) AS 'Thursday',\n",
    "COUNT(CASE WHEN strftime('%w',pickup_datetime) IS '5' THEN 1 END) AS 'Friday',\n",
    "COUNT(CASE WHEN strftime('%w',pickup_datetime) IS '6' THEN 1 END) AS 'Saturday'\n",
    "FROM uber\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "25bdf691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(25834, 24681, 27526, 28328, 29338, 30166, 29599)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.execute(QUERY_2).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8c415563",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_2, \"popular_uber_days.sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b09401",
   "metadata": {},
   "source": [
    "### Query 3\n",
    "\n",
    "Here we will find the 95% percentile of distance traveled for all hired trips during July 2013 by forming a union between our Yellow Taxi data and Uber data and finding what distance encompasses 95% of all rides.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6e3b0c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_3 = \"\"\"\n",
    "SELECT distance\n",
    "FROM taxi\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT distance\n",
    "FROM uber\n",
    "ORDER BY distance DESC\n",
    "LIMIT 1 OFFSET 19055\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b0321c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(10.438645448958848,)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.execute(QUERY_3).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "82810039",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_3, \"rides_95th_percentile.sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bd0dd5",
   "metadata": {},
   "source": [
    "### Query 4\n",
    "\n",
    "Here we find the 10 days with the most hired rides and their average distance by filtering our union and grouping by day. We then order by number of rides per day and select the top 10 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9db09a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_4 = \"\"\"\n",
    "SELECT pickup_datetime, AVG(distance)\n",
    "FROM (\n",
    "SELECT *\n",
    "FROM taxi\n",
    "WHERE pickup_datetime LIKE '2009%'\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT *\n",
    "FROM uber\n",
    "WHERE pickup_datetime LIKE '2009%'\n",
    ")\n",
    "GROUP BY pickup_datetime\n",
    "ORDER BY COUNT(pickup_datetime) DESC\n",
    "LIMIT 10\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ecb0a5d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2009-08-21 19:01:00.000000', 2.3004645356549798),\n",
       " ('2009-02-12 12:46:00.000000', 2.06106123864422),\n",
       " ('2009-12-29 19:49:00.000000', 1.359449274955546),\n",
       " ('2009-12-22 11:08:00.000000', 1.0413531499025293),\n",
       " ('2009-12-16 13:05:00.000000', 2.591184118342559),\n",
       " ('2009-12-06 18:24:00.000000', 3.0824503260395963),\n",
       " ('2009-12-01 15:03:00.000000', 2.170797180883661),\n",
       " ('2009-11-23 17:51:00.000000', 2.4266183086888327),\n",
       " ('2009-11-14 00:41:00.000000', 7.162453247895702),\n",
       " ('2009-11-05 23:39:00.000000', 3.015314787749366)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.execute(QUERY_4).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "29582a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_4, \"avg_dist_of_busiest_days_2009.sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbabec66",
   "metadata": {},
   "source": [
    "### Query 5\n",
    "\n",
    "Here we will find what days in 2014 were the windiest and how many rides were taken on those days by grouping our rides by the day, joining with our daily weather reports, and ordering by windiest days.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b2440113",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_5 = \"\"\"\n",
    "SELECT strftime('%Y-%m-%d',pickup_datetime) AS date, COUNT(\n",
    "    (SELECT distance FROM taxi WHERE pickup_datetime LIKE '2014%'\n",
    "    UNION ALL\n",
    "    SELECT distance FROM uber WHERE pickup_datetime LIKE '2014%'\n",
    "    ) \n",
    "    )\n",
    "FROM (\n",
    "SELECT *\n",
    "FROM taxi\n",
    "WHERE pickup_datetime LIKE '2014%'\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT *\n",
    "FROM uber\n",
    "WHERE pickup_datetime LIKE '2014%'\n",
    ") AS a\n",
    "\n",
    "JOIN dailyWeather ON strftime('%Y-%m-%d',a.pickup_datetime)=dailyWeather.id\n",
    "GROUP BY strftime('%Y-%m-%d',pickup_datetime)\n",
    "ORDER BY dailyWeather.DailyWindSpeed DESC\n",
    "LIMIT 10\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "47462cca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2014-03-13', 197),\n",
       " ('2014-01-07', 162),\n",
       " ('2014-01-02', 125),\n",
       " ('2014-02-13', 119),\n",
       " ('2014-03-26', 170),\n",
       " ('2014-03-29', 199),\n",
       " ('2014-12-07', 156),\n",
       " ('2014-12-09', 153),\n",
       " ('2014-12-08', 154),\n",
       " ('2014-11-02', 147)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.execute(QUERY_5).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ec478f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_5, \"windy_days_with_trips.sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292c72bf",
   "metadata": {},
   "source": [
    "### Query 6\n",
    "\n",
    "Here we will find the number of rides taken, precipitation and wind speed for every hour in the two week period surrounding Hurricane Sandy's presence in NYC. We group our rides by hour and limit our search to the desired time period.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2fb04f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_6 = \"\"\"\n",
    "SELECT COUNT(\n",
    "    (SELECT distance FROM taxi WHERE (\n",
    "       pickup_datetime LIKE '2012-10-22%'\n",
    "    OR pickup_datetime LIKE '2012-10-23%'\n",
    "    OR pickup_datetime LIKE '2012-10-24%'\n",
    "    OR pickup_datetime LIKE '2012-10-25%'\n",
    "    OR pickup_datetime LIKE '2012-10-26%'\n",
    "    OR pickup_datetime LIKE '2012-10-27%'\n",
    "    OR pickup_datetime LIKE '2012-10-28%'\n",
    "    OR pickup_datetime LIKE '2012-10-29%'\n",
    "    OR pickup_datetime LIKE '2012-10-30%'\n",
    "    OR pickup_datetime LIKE '2012-10-31%'\n",
    "    OR pickup_datetime LIKE '2012-11-01%'\n",
    "    OR pickup_datetime LIKE '2012-11-02%'\n",
    "    OR pickup_datetime LIKE '2012-11-03%'\n",
    "    OR pickup_datetime LIKE '2012-11-04%'\n",
    "    OR pickup_datetime LIKE '2012-11-05%'\n",
    "    OR pickup_datetime LIKE '2012-11-06%'\n",
    "    )\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT distance FROM uber WHERE (\n",
    "       pickup_datetime LIKE '2012-10-22%'\n",
    "    OR pickup_datetime LIKE '2012-10-23%'\n",
    "    OR pickup_datetime LIKE '2012-10-24%'\n",
    "    OR pickup_datetime LIKE '2012-10-25%'\n",
    "    OR pickup_datetime LIKE '2012-10-26%'\n",
    "    OR pickup_datetime LIKE '2012-10-27%'\n",
    "    OR pickup_datetime LIKE '2012-10-28%'\n",
    "    OR pickup_datetime LIKE '2012-10-29%'\n",
    "    OR pickup_datetime LIKE '2012-10-30%'\n",
    "    OR pickup_datetime LIKE '2012-10-31%'\n",
    "    OR pickup_datetime LIKE '2012-11-01%'\n",
    "    OR pickup_datetime LIKE '2012-11-02%'\n",
    "    OR pickup_datetime LIKE '2012-11-03%'\n",
    "    OR pickup_datetime LIKE '2012-11-04%'\n",
    "    OR pickup_datetime LIKE '2012-11-05%'\n",
    "    OR pickup_datetime LIKE '2012-11-06%'\n",
    "    ) \n",
    "    )), hourlyWeather.HourlyPrecipitation, hourlyWeather.HourlyWindSpeed\n",
    "FROM (\n",
    "    SELECT * FROM taxi WHERE (\n",
    "       pickup_datetime LIKE '2012-10-22%'\n",
    "    OR pickup_datetime LIKE '2012-10-23%'\n",
    "    OR pickup_datetime LIKE '2012-10-24%'\n",
    "    OR pickup_datetime LIKE '2012-10-25%'\n",
    "    OR pickup_datetime LIKE '2012-10-26%'\n",
    "    OR pickup_datetime LIKE '2012-10-27%'\n",
    "    OR pickup_datetime LIKE '2012-10-28%'\n",
    "    OR pickup_datetime LIKE '2012-10-29%'\n",
    "    OR pickup_datetime LIKE '2012-10-30%'\n",
    "    OR pickup_datetime LIKE '2012-10-31%'\n",
    "    OR pickup_datetime LIKE '2012-11-01%'\n",
    "    OR pickup_datetime LIKE '2012-11-02%'\n",
    "    OR pickup_datetime LIKE '2012-11-03%'\n",
    "    OR pickup_datetime LIKE '2012-11-04%'\n",
    "    OR pickup_datetime LIKE '2012-11-05%'\n",
    "    OR pickup_datetime LIKE '2012-11-06%'\n",
    "    )\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT * FROM uber WHERE (\n",
    "       pickup_datetime LIKE '2012-10-22%'\n",
    "    OR pickup_datetime LIKE '2012-10-23%'\n",
    "    OR pickup_datetime LIKE '2012-10-24%'\n",
    "    OR pickup_datetime LIKE '2012-10-25%'\n",
    "    OR pickup_datetime LIKE '2012-10-26%'\n",
    "    OR pickup_datetime LIKE '2012-10-27%'\n",
    "    OR pickup_datetime LIKE '2012-10-28%'\n",
    "    OR pickup_datetime LIKE '2012-10-29%'\n",
    "    OR pickup_datetime LIKE '2012-10-30%'\n",
    "    OR pickup_datetime LIKE '2012-10-31%'\n",
    "    OR pickup_datetime LIKE '2012-11-01%'\n",
    "    OR pickup_datetime LIKE '2012-11-02%'\n",
    "    OR pickup_datetime LIKE '2012-11-03%'\n",
    "    OR pickup_datetime LIKE '2012-11-04%'\n",
    "    OR pickup_datetime LIKE '2012-11-05%'\n",
    "    OR pickup_datetime LIKE '2012-11-06%'\n",
    "    )\n",
    ") AS a\n",
    "JOIN hourlyWeather ON strftime('%Y-%m-%d %H',a.pickup_datetime)=strftime('%Y-%m-%d %H',hourlyWeather.date)\n",
    "GROUP BY strftime('%Y-%m-%d %H',a.pickup_datetime)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "756c37ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 0.0, 7.0),\n",
       " (2, 0.0, 5.0),\n",
       " (2, 0.0, 7.0),\n",
       " (1, 0.0, 0.0),\n",
       " (1, 0.0, 0.0),\n",
       " (6, 0.0, 5.0),\n",
       " (10, 0.0, 3.0),\n",
       " (8, 0.0, 3.0),\n",
       " (9, 0.0, 5.0),\n",
       " (8, 0.0, 11.0),\n",
       " (7, 0.0, 7.0),\n",
       " (7, 0.0, 6.0),\n",
       " (8, 0.0, 3.0),\n",
       " (8, 0.0, 7.0),\n",
       " (15, 0.0, 5.0),\n",
       " (4, 0.0, 5.0),\n",
       " (11, 0.0, 3.0),\n",
       " (5, 0.0, 0.0),\n",
       " (10, 0.0, 3.0),\n",
       " (5, 0.0, 3.0),\n",
       " (12, 0.02, 0.0),\n",
       " (11, 0.01, 0.0),\n",
       " (2, 0.0, 3.0),\n",
       " (1, 0.0, 5.0),\n",
       " (4, 0.0, 5.0),\n",
       " (1, 0.0, 7.0),\n",
       " (1, 0.0, 7.0),\n",
       " (5, 0.0, 6.0),\n",
       " (1, 0.0, 3.0),\n",
       " (4, 0.0, 7.0),\n",
       " (9, 0.0, 7.0),\n",
       " (7, 0.0, 8.0),\n",
       " (3, 0.0, 7.0),\n",
       " (9, 0.0, 7.0),\n",
       " (5, 0.0, 8.0),\n",
       " (8, 0.0, 5.0),\n",
       " (6, 0.0, 7.0),\n",
       " (14, 0.0, 8.0),\n",
       " (17, 0.0, 3.0),\n",
       " (12, 0.0, 5.0),\n",
       " (7, 0.0, 6.0),\n",
       " (9, 0.0, 0.0),\n",
       " (8, 0.0, 3.0),\n",
       " (3, 0.0, 3.0),\n",
       " (2, 0.0, 3.0),\n",
       " (1, 0.0, 6.0),\n",
       " (3, 0.0, 0.0),\n",
       " (7, 0.0, 5.0),\n",
       " (9, 0.0, 6.0),\n",
       " (9, 0.0, 5.0),\n",
       " (7, 0.0, 3.0),\n",
       " (9, 0.0, 6.0),\n",
       " (13, 0.0, 0.0),\n",
       " (10, 0.0, 6.0),\n",
       " (10, 0.0, 0.0),\n",
       " (11, 0.0, 5.0),\n",
       " (7, 0.0, 5.0),\n",
       " (4, 0.0, 0.0),\n",
       " (8, 0.0, 3.0),\n",
       " (12, 0.0, 0.0),\n",
       " (4, 0.0, 0.0),\n",
       " (14, 0.0, 3.0),\n",
       " (14, 0.0, 3.0),\n",
       " (14, 0.0, 3.0),\n",
       " (14, 0.0, 0.0),\n",
       " (8, 0.0, 5.0),\n",
       " (8, 0.0, 0.0),\n",
       " (3, 0.0, 0.0),\n",
       " (3, 0.0, 3.0),\n",
       " (5, 0.0, 0.0),\n",
       " (1, 0.0, 0.0),\n",
       " (3, 0.0, 0.0),\n",
       " (9, 0.0, 3.0),\n",
       " (5, 0.0, 3.0),\n",
       " (9, 0.0, 3.0),\n",
       " (7, 0.0, 3.0),\n",
       " (9, 0.0, 3.0),\n",
       " (4, 0.0, 0.0),\n",
       " (10, 0.0, 3.0),\n",
       " (8, 0.0, 3.0),\n",
       " (9, 0.0, 0.0),\n",
       " (8, 0.0, 0.0),\n",
       " (4, 0.0, 0.0),\n",
       " (8, 0.0, 0.0),\n",
       " (9, 0.0, 0.0),\n",
       " (15, 0.0, 3.0),\n",
       " (11, 0.0, 3.0),\n",
       " (9, 0.0, 0.0),\n",
       " (7, 0.0, 0.0),\n",
       " (8, 0.0, 3.0),\n",
       " (7, 0.0, 0.0),\n",
       " (7, 0.0, 3.0),\n",
       " (6, 0.0, 0.0),\n",
       " (2, 0.0, 6.0),\n",
       " (3, 0.0, 6.0),\n",
       " (2, 0.0, 6.0),\n",
       " (1, 0.0, 3.0),\n",
       " (6, 0.0, 8.0),\n",
       " (8, 0.0, 6.0),\n",
       " (7, 0.0, 9.0),\n",
       " (5, 0.0, 6.0),\n",
       " (8, 0.0, 8.0),\n",
       " (4, 0.0, 8.0),\n",
       " (7, 0.0, 10.0),\n",
       " (7, 0.0, 10.0),\n",
       " (11, 0.0, 7.0),\n",
       " (13, 0.0, 7.0),\n",
       " (12, 0.0, 7.0),\n",
       " (7, 0.0, 9.0),\n",
       " (12, 0.0, 7.0),\n",
       " (13, 0.0, 9.0),\n",
       " (16, 0.0, 9.0),\n",
       " (13, 0.0, 8.0),\n",
       " (10, 0.0, 11.0),\n",
       " (13, 0.0, 8.0),\n",
       " (5, 0.0, 8.0),\n",
       " (6, 0.0, 9.0),\n",
       " (6, 0.0, 10.0),\n",
       " (2, 0.0, 11.0),\n",
       " (4, 0.0, 11.0),\n",
       " (1, 0.0, 11.0),\n",
       " (7, 0.0, 11.0),\n",
       " (4, 0.0, 10.0),\n",
       " (10, 0.0, 8.0),\n",
       " (7, 0.0, 7.0),\n",
       " (5, 0.0, 13.0),\n",
       " (8, 0.0, 13.0),\n",
       " (8, 0.0, 13.0),\n",
       " (6, 0.0, 16.0),\n",
       " (9, 0.0, 11.0),\n",
       " (8, 0.0, 15.0),\n",
       " (6, 0.0, 14.0),\n",
       " (6, 0.0, 16.0),\n",
       " (6, 0.0, 14.0),\n",
       " (4, 0.0, 16.0),\n",
       " (2, 0.0, 14.0),\n",
       " (1, 0.0, 16.0),\n",
       " (1, 0.0, 13.0),\n",
       " (1, 0.0, 15.0),\n",
       " (1, 0.02, 17.0),\n",
       " (1, 0.0, 16.0),\n",
       " (6, 0.01, 15.0),\n",
       " (35, 0.02, 15.0),\n",
       " (6, 0.02, 24.0),\n",
       " (21, 0.03, 23.0),\n",
       " (6, 0.07, 26.0),\n",
       " (8, 0.04, 29.0),\n",
       " (8, 0.02, 21.0),\n",
       " (1, 0.01, 25.0),\n",
       " (1, 0.0, 15.0),\n",
       " (2, 0.03, 13.0),\n",
       " (2, 0.01, 11.0),\n",
       " (6, 0.01, 15.0),\n",
       " (9, 0.02, 8.0),\n",
       " (4, 0.0, 7.0),\n",
       " (4, 0.0, 7.0),\n",
       " (5, 0.0, 6.0),\n",
       " (6, 0.01, 3.0),\n",
       " (4, 0.0, 5.0),\n",
       " (5, 0.0, 5.0),\n",
       " (5, 0.0, 3.0),\n",
       " (3, 0.0, 7.0),\n",
       " (5, 0.0, 5.0),\n",
       " (5, 0.0, 7.0),\n",
       " (5, 0.0, 9.0),\n",
       " (1, 0.0, 8.0),\n",
       " (3, 0.0, 6.0),\n",
       " (3, 0.0, 8.0),\n",
       " (3, 0.0, 7.0),\n",
       " (3, 0.0, 8.0),\n",
       " (4, 0.0, 3.0),\n",
       " (3, 0.0, 5.0),\n",
       " (1, 0.0, 3.0),\n",
       " (5, 0.0, 6.0),\n",
       " (4, 0.0, 5.0),\n",
       " (5, 0.0, 3.0),\n",
       " (3, 0.0, 5.0),\n",
       " (4, 0.0, 5.0),\n",
       " (10, 0.0, 3.0),\n",
       " (7, 0.0, 9.0),\n",
       " (1, 0.0, 7.0),\n",
       " (5, 0.0, 7.0),\n",
       " (6, 0.0, 6.0),\n",
       " (2, 0.0, 3.0),\n",
       " (4, 0.0, 3.0),\n",
       " (3, 0.0, 3.0),\n",
       " (2, 0.0, 7.0),\n",
       " (3, 0.0, 13.0),\n",
       " (7, 0.0, 7.0),\n",
       " (5, 0.0, 3.0),\n",
       " (9, 0.0, 6.0),\n",
       " (9, 0.0, 6.0),\n",
       " (9, 0.0, 11.0),\n",
       " (4, 0.0, 8.0),\n",
       " (7, 0.0, 8.0),\n",
       " (5, 0.0, 5.0),\n",
       " (4, 0.0, 5.0),\n",
       " (10, 0.0, 9.0),\n",
       " (7, 0.0, 3.0),\n",
       " (6, 0.0, 5.0),\n",
       " (2, 0.0, 8.0),\n",
       " (7, 0.0, 5.0),\n",
       " (3, 0.0, 0.0),\n",
       " (6, 0.0, 5.0),\n",
       " (2, 0.0, 7.0),\n",
       " (1, 0.0, 3.0),\n",
       " (1, 0.0, 3.0),\n",
       " (1, 0.0, 5.0),\n",
       " (2, 0.0, 6.0),\n",
       " (6, 0.0, 5.0),\n",
       " (7, 0.0, 9.0),\n",
       " (6, 0.0, 7.0),\n",
       " (4, 0.0, 7.0),\n",
       " (2, 0.0, 6.0),\n",
       " (11, 0.0, 6.0),\n",
       " (5, 0.0, 5.0),\n",
       " (7, 0.0, 11.0),\n",
       " (3, 0.0, 8.0),\n",
       " (1, 0.0, 9.0),\n",
       " (6, 0.0, 7.0),\n",
       " (10, 0.0, 9.0),\n",
       " (6, 0.0, 7.0),\n",
       " (8, 0.0, 8.0),\n",
       " (4, 0.0, 8.0),\n",
       " (9, 0.0, 7.0),\n",
       " (6, 0.0, 7.0),\n",
       " (11, 0.0, 7.0),\n",
       " (2, 0.0, 7.0),\n",
       " (1, 0.0, 8.0),\n",
       " (2, 0.0, 8.0),\n",
       " (2, 0.0, 7.0),\n",
       " (3, 0.0, 6.0),\n",
       " (3, 0.0, 10.0),\n",
       " (4, 0.0, 13.0),\n",
       " (7, 0.0, 6.0),\n",
       " (10, 0.0, 13.0),\n",
       " (5, 0.0, 13.0),\n",
       " (4, 0.0, 8.0),\n",
       " (9, 0.0, 8.0),\n",
       " (6, 0.0, 7.0),\n",
       " (3, 0.0, 10.0),\n",
       " (10, 0.0, 9.0),\n",
       " (8, 0.0, 9.0),\n",
       " (9, 0.0, 13.0),\n",
       " (11, 0.0, 10.0),\n",
       " (11, 0.0, 9.0),\n",
       " (11, 0.0, 0.0),\n",
       " (18, 0.0, 7.0),\n",
       " (12, 0.0, 9.0),\n",
       " (17, 0.0, 7.0),\n",
       " (9, 0.0, 7.0),\n",
       " (3, 0.0, 7.0),\n",
       " (3, 0.0, 8.0),\n",
       " (4, 0.0, 6.0),\n",
       " (3, 0.0, 3.0),\n",
       " (6, 0.0, 7.0),\n",
       " (3, 0.0, 9.0),\n",
       " (7, 0.0, 6.0),\n",
       " (10, 0.0, 8.0),\n",
       " (9, 0.0, 8.0),\n",
       " (10, 0.0, 7.0),\n",
       " (4, 0.0, 7.0),\n",
       " (8, 0.0, 5.0),\n",
       " (10, 0.0, 5.0),\n",
       " (8, 0.0, 7.0),\n",
       " (7, 0.0, 7.0),\n",
       " (6, 0.0, 6.0),\n",
       " (3, 0.0, 5.0),\n",
       " (5, 0.0, 0.0),\n",
       " (3, 0.0, 5.0),\n",
       " (1, 0.0, 3.0),\n",
       " (2, 0.0, 6.0),\n",
       " (3, 0.0, 8.0),\n",
       " (9, 0.0, 6.0),\n",
       " (5, 0.0, 7.0),\n",
       " (14, 0.0, 3.0),\n",
       " (9, 0.0, 3.0),\n",
       " (6, 0.0, 3.0),\n",
       " (6, 0.0, 5.0),\n",
       " (5, 0.0, 3.0),\n",
       " (5, 0.0, 8.0),\n",
       " (21, 0.0, 5.0),\n",
       " (16, 0.0, 5.0),\n",
       " (9, 0.0, 0.0),\n",
       " (13, 0.0, 3.0),\n",
       " (11, 0.0, 7.0),\n",
       " (5, 0.0, 6.0),\n",
       " (3, 0.0, 9.0),\n",
       " (6, 0.0, 6.0),\n",
       " (3, 0.0, 5.0),\n",
       " (3, 0.0, 8.0),\n",
       " (1, 0.0, 10.0),\n",
       " (1, 0.0, 6.0),\n",
       " (5, 0.0, 5.0),\n",
       " (1, 0.0, 7.0),\n",
       " (9, 0.0, 7.0),\n",
       " (9, 0.0, 8.0),\n",
       " (8, 0.0, 3.0),\n",
       " (6, 0.0, 6.0),\n",
       " (5, 0.0, 7.0),\n",
       " (10, 0.0, 5.0),\n",
       " (12, 0.0, 0.0),\n",
       " (9, 0.0, 6.0),\n",
       " (10, 0.0, 6.0),\n",
       " (8, 0.0, 5.0),\n",
       " (10, 0.0, 5.0),\n",
       " (8, 0.0, 3.0),\n",
       " (16, 0.0, 3.0),\n",
       " (7, 0.0, 7.0),\n",
       " (9, 0.0, 7.0),\n",
       " (8, 0.0, 7.0),\n",
       " (8, 0.0, 3.0)]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.execute(QUERY_6).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1d33ed70",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_6, \"hurricane_rides_and_weather_by_hour.sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ced42",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing the Data\n",
    "\n",
    "_A checklist of requirements to keep you on track. Remove this whole cell before submitting the project. The order of these tasks aren't necessarily the order in which they need to be done. It's okay to do them in an order that makes sense to you._\n",
    "\n",
    "* [ ] Create an appropriate visualization for the first query/question in part 3\n",
    "* [ ] Create a visualization that shows the average distance traveled per month (regardless of year - so group by each month). Include the 90% confidence interval around the mean in the visualization\n",
    "* [ ] Define three lat/long coordinate boxes around the three major New York airports: LGA, JFK, and EWR (you can use bboxfinder to help). Create a visualization that compares what day of the week was most popular for drop offs for each airport.\n",
    "* [ ] Create a heatmap of all hired trips over a map of the area. Consider using KeplerGL or another library that helps generate geospatial visualizations.\n",
    "* [ ] Create a scatter plot that compares tip amount versus distance.\n",
    "* [ ] Create another scatter plot that compares tip amount versus precipitation amount.\n",
    "\n",
    "_Be sure these cells are executed so that the visualizations are rendered when the notebook is submitted._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9eef42",
   "metadata": {},
   "source": [
    "### Visualization N\n",
    "\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._\n",
    "\n",
    "_Repeat for each visualization._\n",
    "\n",
    "_The example below makes use of the `matplotlib` library. There are other libraries, including `pandas` built-in plotting library, kepler for geospatial data representation, `seaborn`, and others._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de8394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a more descriptive name for your function\n",
    "def plot_visual_n(dataframe):\n",
    "    figure, axes = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    values = \"...\"  # use the dataframe to pull out values needed to plot\n",
    "    \n",
    "    # you may want to use matplotlib to plot your visualizations;\n",
    "    # there are also many other plot types (other \n",
    "    # than axes.plot) you can use\n",
    "    axes.plot(values, \"...\")\n",
    "    # there are other methods to use to label your axes, to style \n",
    "    # and set up axes labels, etc\n",
    "    axes.set_title(\"Some Descriptive Title\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847ced2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_n():\n",
    "    # Query SQL database for the data needed.\n",
    "    # You can put the data queried into a pandas dataframe, if you wish\n",
    "    raise NotImplemented()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c63e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dataframe = get_data_for_visual_n()\n",
    "plot_visual_n(some_dataframe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
